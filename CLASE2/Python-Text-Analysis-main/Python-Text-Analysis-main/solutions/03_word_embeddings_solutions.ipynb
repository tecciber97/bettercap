{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f37149-c4d4-4bb2-be40-33caccccdc92",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Part 3 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428bef9b-2ce9-456c-a260-9b72d79d6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c923bb-7286-4d55-8002-684343a5a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4a748-d43e-4c76-8eaa-ad504e5c3a51",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 1: Dosen't Match\n",
    "\n",
    "Now it's your turn! In the following cell, we have prepared a list of coffee-noun pairs, i.e., the word \"coffee\" is paired with a specific coffee drink. Let's find out which coffee drink is considered most similar to \"coffee,\" and which one is not. \n",
    "\n",
    "Complete the for loop (two cells below) to calculate the cosine similarity between each pair of words, i.e., make use of the `similarity` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63c4f076-005a-4176-93dd-60e681f6b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_nouns = [\n",
    "    ('coffee', 'espresso'),\n",
    "    ('coffee', 'cappuccino'),\n",
    "    ('coffee', 'latte'),\n",
    "    ('coffee', 'americano'),\n",
    "    ('coffee', 'irish'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfa15ad5-b3fe-4871-8c07-bfa1bac58e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee, espresso, 0.6616826057434082\n",
      "coffee, cappuccino, 0.662549614906311\n",
      "coffee, latte, 0.6049396395683289\n",
      "coffee, americano, 0.33290809392929077\n",
      "coffee, irish, 0.16667571663856506\n"
     ]
    }
   ],
   "source": [
    "# Get cosine similarities between each pair\n",
    "for w1, w2 in coffee_nouns:\n",
    "    similarity = wv.similarity(w1, w2)\n",
    "    print(f\"{w1}, {w2}, {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df5c2a-3980-43b2-93c0-5b6c1a6b6912",
   "metadata": {},
   "source": [
    "Next, let's investigate verbs commonly associated with coffee-making. Take a look at the use case for the [`doesnt_match`](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#word2vec-demo) function and then use it to identify the verb that does not seem to belong.\n",
    "\n",
    "Feel free to add more verbs to the list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfe8838e-c48f-49f0-9a0c-f135e1fb7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_verbs = ['brew', 'drip', 'pour', 'make', 'grind', 'roast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbc36a9d-08c2-47ca-8564-0c3d76fde4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the word that doesn't belong to the list\n",
    "verb_dosent_match = wv.doesnt_match(coffee_verbs)\n",
    "verb_dosent_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ac569-8889-4dbd-aa5d-b0ae7314a0aa",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 2: Woman is to Homemaker?\n",
    "\n",
    "[Bolukbasi et al. (2016)](https://arxiv.org/pdf/1607.06520) is a thorough investigation of gender bias present in word embeddings, and they primarily focus on word analogies, especially those that reveal gender stereotyping! Let run a couple examples discussed in the paper, using the `most_similiar` function we've just learned. \n",
    "\n",
    "The following code block contains a few examples we can pass to the `positive` argument: we want the output to be similar to, for example, `woman` and `chairman`, and in the meantime, we are also specificying that it should be dissimilar to `man`. We'll print the top result by indexing to the 0th item. \n",
    "\n",
    "Let's complete the following for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4082d7e-9b2e-43dc-b7fa-b847889760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pair = [['woman', 'chairman'],\n",
    "                 ['woman', 'doctor'], \n",
    "                 ['woman', 'computer_programmer']]\n",
    "negative_word = 'man'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc9a002-fd48-4f8f-b677-842c8538a560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man is to chairman as woman is to chairwoman\n",
      "man is to doctor as woman is to gynecologist\n",
      "man is to computer_programmer as woman is to homemaker\n"
     ]
    }
   ],
   "source": [
    "# Get the most similar word given positive and negative examples\n",
    "for example in positive_pair:\n",
    "    result = wv.most_similar(positive=example, negative=negative_word)\n",
    "    print(f\"man is to {example[1]} as woman is to {result[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0142b3-545b-4f52-8e7b-2711e20abbd6",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 3: Construct a Semantic Axis\n",
    "\n",
    "Now it's your turn! We have two sets of pole words for \"female\" and \"male\". These are example words tested in Bolukbasi et al., 2016. We will get the embeddings for these words from glove to calculate the gender axis. \n",
    "\n",
    "The cell for the function `get_semaxis` provides some starting code. Complete the function. If everything runs, the embedding size of the semantic axis should be the same as the size of the input vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72fb395-796d-410a-a3fa-f14c55b36f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bac1297-5aca-4cdf-bd2b-bd0c71cfd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two sets of pole words (examples from Bolukbasi et al., 2016)\n",
    "female = ['she', 'woman', 'female', 'daughter', 'mother', 'girl']\n",
    "male = ['he', 'man', 'male', 'son', 'father', 'boy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "815d6225-015a-4ad9-98e6-f05a4aabbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semaxis(list1, list2, model, embedding_size):\n",
    "    '''Calculate the embedding of a semantic axis given two lists of pole words.'''\n",
    "\n",
    "    # STEP 1: Get the embeddings for terms in each list\n",
    "    v_plus = [model[term] for term in list1]\n",
    "    v_minus = [model[term] for term in list2]\n",
    "\n",
    "    # Step 2: Calculate the mean embeddings for each list\n",
    "    v_plus_mean = np.mean(v_plus, axis=0)\n",
    "    v_minus_mean = np.mean(v_minus, axis=0)\n",
    "\n",
    "    # Step 3: Get the difference between two means\n",
    "    sem_axis = v_plus_mean - v_minus_mean\n",
    "\n",
    "    # Sanity check\n",
    "    assert sem_axis.size == embedding_size\n",
    "    \n",
    "    return sem_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afe27243-1c45-4433-8f33-559c121c98b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08418201,  0.30625182, -0.23662159,  0.02026337, -0.00296998,\n",
       "        0.6195349 ,  0.01208681,  0.06963003,  0.49099812, -0.20878893,\n",
       "        0.00934163, -0.44707334,  0.48806185,  0.19471335,  0.20141667,\n",
       "        0.0832995 , -0.4245833 , -0.08612835,  0.47612852, -0.05129966,\n",
       "        0.31475997,  0.49075842,  0.12465019,  0.26685053,  0.29776838,\n",
       "        0.14211655, -0.09953564,  0.2320785 , -0.01026282, -0.30585438,\n",
       "       -0.1335001 ,  0.21605133,  0.10961549, -0.03373036, -0.13584831,\n",
       "       -0.12131716, -0.14671612, -0.04348468,  0.06151834, -0.3654362 ,\n",
       "       -0.06193466, -0.17093089,  0.5058871 , -0.44872418,  0.05962732,\n",
       "       -0.18274659,  0.24432765, -0.3396697 ,  0.00442566,  0.10554916],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plug in the gender lists to calculate the semantic axis for gender\n",
    "gender_axis = get_semaxis(list1=female, \n",
    "                          list2=male, \n",
    "                          model=glove, \n",
    "                          embedding_size=50)\n",
    "gender_axis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
